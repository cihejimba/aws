# https://aws.plainenglish.io/using-aws-lambda-with-python-to-auto-write-to-a-dynamodb-table-from-s3-ee01da85094
# https://aws.amazon.com/fr/blogs/database/migrate-delimited-files-from-amazon-s3-to-an-amazon-dynamodb-nosql-table-using-aws-database-migration-service-and-aws-cloudformation/


lambda iam role
access to S3
access to DynamoDB
access to CloudWatch logs


import boto3
import json
s3_client = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')
def lambda_handler(event, context):
    bucket = event['Records'][0]['s3']['bucket']['name']
    json_file_name = event['Records'][0]['s3']['object']['key']
    json_object = s3_client.get_object(Bucket=bucket,Key=json_file_name)
    jsonFileReader = json_object['Body'].read()
    jsonDict = json.loads(jsonFileReader)
    table = dynamodb.Table('employees')
    table.put_item(Item=jsonDict)

